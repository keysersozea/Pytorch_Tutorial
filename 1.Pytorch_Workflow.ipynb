{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore an example PyTorch end to end workflow.\n",
    "\n",
    "What this notebook covering:\n",
    "\n",
    "1. Data preparing and loading\n",
    "2. Building models\n",
    "3. Fitting the model to data (training)\n",
    "4. Making predictions and evaluating a model (inference)\n",
    "5. Saving and loading a model\n",
    "6. Putting it all together\n",
    "\n",
    "https://pytorch.org/docs/stable/nn.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data (preparing and learning)\n",
    "\n",
    "Data can be almost anything... in machine learning\n",
    "\n",
    "Machine learning is a game of two parts:\n",
    "\n",
    "1. Get data into a numerical representation\n",
    "2. Build a model to learn patterns in that numerical representations\n",
    "\n",
    "To showcase this, let's create some *known* data using the linear regression formula.\n",
    "We'll use a linear regression formula to make a straight line with *known* **parameters**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 50)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create 'known' parameters\n",
    "weight = 0.7\n",
    "bias = 0.3\n",
    "\n",
    "# Create\n",
    "start = 0\n",
    "end = 1\n",
    "step = 0.02\n",
    "X = torch.arange(start, end, step).unsqueeze(dim=1)\n",
    "y = X * weight + bias\n",
    "\n",
    "len(X), len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting data into training and test sets\n",
    "\n",
    "Let's create a training and test set with our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing\n",
    "split = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:split], X[split:]\n",
    "y_train, y_test = y[:split], y[split:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionModel(nn.Module): # Subclass nn.Module contains all the building blocks for neural networks\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Initialize model parameters   --    These could be different layers from torch.nn, single parameters, etc)\n",
    "        self.weight = nn.Parameter(torch.rand(1,  # Modelin öğrenebilir parametreleri nn.Parameter ile tanımlanmalı\n",
    "                                              requires_grad=True,  # Pytorch will keep track gradient of this for use with torch.autograd and gradient descent\n",
    "                                              dtype=float))\n",
    "        self.bias = nn.Parameter(torch.rand(1,\n",
    "                                              requires_grad=True,  \n",
    "                                              dtype=float))\n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:    # Any subclass of nn.Module needs to override forward() (this defines forward computation of the model)\n",
    "        return self.weight * X + self.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch model building essentials\n",
    "\n",
    "PyTorch has four (give or take) essential modules you can use to create almost any kind of neural network you can imagine.\n",
    "\n",
    "They are [`torch.nn`](https://pytorch.org/docs/stable/nn.html), [`torch.optim`](https://pytorch.org/docs/stable/optim.html), [`torch.utils.data.Dataset`](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) and [`torch.utils.data.DataLoader`](https://pytorch.org/docs/stable/data.html). For now, we'll focus on the first two and get to the other two later (though you may be able to guess what they do).\n",
    "\n",
    "| PyTorch module | What does it do? |\n",
    "| ----- | ----- |\n",
    "| [`torch.nn`](https://pytorch.org/docs/stable/nn.html) | Contains all of the building blocks for computational graphs (essentially a series of computations executed in a particular way). |\n",
    "| [`torch.nn.Parameter`](https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#parameter) | Stores tensors that can be used with `nn.Module`. If `requires_grad=True` gradients (used for updating model parameters via [**gradient descent**](https://ml-cheatsheet.readthedocs.io/en/latest/gradient_descent.html))  are calculated automatically, this is often referred to as \"autograd\".  | \n",
    "| [`torch.nn.Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module) | The base class for all neural network modules, all the building blocks for neural networks are subclasses. If you're building a neural network in PyTorch, your models should subclass `nn.Module`. Requires a `forward()` method be implemented. | \n",
    "| [`torch.optim`](https://pytorch.org/docs/stable/optim.html) | Contains various optimization algorithms (these tell the model parameters stored in `nn.Parameter` how to best change to improve gradient descent and in turn reduce the loss). | \n",
    "| `def forward()` | All `nn.Module` subclasses require a `forward()` method, this defines the computation that will take place on the data passed to the particular `nn.Module` (e.g. the linear regression formula above). |\n",
    "\n",
    "If the above sounds complex, think of like this, almost everything in a PyTorch neural network comes from `torch.nn`,\n",
    "* `nn.Module` contains the larger building blocks (layers)\n",
    "* `nn.Parameter` contains the smaller parameters like weights and biases (put these together to make `nn.Module`(s))\n",
    "* `forward()` tells the larger blocks how to make calculations on inputs (tensors full of data) within  `nn.Module`(s)\n",
    "* `torch.optim` contains optimization methods on how to improve the parameters within `nn.Parameter` to better represent input data \n",
    "\n",
    "> **Resource:** See more of these essential modules and their uses cases in the [PyTorch Cheat Sheet](https://pytorch.org/tutorials/beginner/ptcheat.html). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the contents of a PyTorch model\n",
    "\n",
    "Now we've got these out of the way, let's create a model instance with the class we've made and check its parameters using .parameters()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([0.0582], dtype=torch.float64, requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.0629], dtype=torch.float64, requires_grad=True)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "# Create a random seed\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Create a instance of a model\n",
    "model_0 = LinearRegressionModel()\n",
    "\n",
    "# Check out the parameters\n",
    "list(model_0.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight', tensor([0.0582], dtype=torch.float64)),\n",
       "             ('bias', tensor([0.0629], dtype=torch.float64))])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List named parameters\n",
    "model_0.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making predictions using `torch.inference_mode()`\n",
    "\n",
    "To check this we can pass it the test data `X_test` to see how closely it predicts `y_test`.\n",
    "\n",
    "When we pass data to our model, it'll go through the model's `forward()` method and produce a result using the computation we've defined.\n",
    "\n",
    "Let's make some predictions.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1094],\n",
       "        [0.1106],\n",
       "        [0.1118],\n",
       "        [0.1129],\n",
       "        [0.1141],\n",
       "        [0.1152],\n",
       "        [0.1164],\n",
       "        [0.1176],\n",
       "        [0.1187],\n",
       "        [0.1199]], dtype=torch.float64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions with model\n",
    "with torch.inference_mode():\n",
    "    y.preds = model_0(X_test)\n",
    "y.preds \n",
    "# Results are very bad, since model is not trained\n",
    "\n",
    "# Note: in older PyTorch code you might also see torch.no_grad()\n",
    "# with torch.no_grad():\n",
    "#   y_preds = model_0(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the name suggests, `torch.inference_mode()` is used when using a model for inference (making predictions).\n",
    "\n",
    "`torch.inference_mode()` turns off a bunch of things (like gradient tracking, which is necessary for training but not for inference) to make forward-passes (data going through the forward() method) faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can directly use `y.preds = model_0(X_test)` but using `torch.inference_mode()` is more efficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train model\n",
    "\n",
    "The whole idea of training is for a model to move from some *unknown* paramters (these may be random) to *known* parameters.\n",
    "\n",
    "Or in other words from a poor representation of the data to a better representation of the data.\n",
    "\n",
    "One way to measure how poor or how wrong your models predictions are is to use a loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a loss function and optimizer in PyTorch\n",
    "\n",
    "For our model to update its parameters on its own, we'll need to add a few more things to our recipe.\n",
    "\n",
    "And that's a **loss function** as well as an **optimizer**.\n",
    "\n",
    "The rolls of these are: \n",
    "\n",
    "| Function | What does it do? | Where does it live in PyTorch? | Common values |\n",
    "| ----- | ----- | ----- | ----- |\n",
    "| **Loss function** | Measures how wrong your models predictions (e.g. `y_preds`) are compared to the truth labels (e.g. `y_test`). Lower the better. | PyTorch has plenty of built-in loss functions in [`torch.nn`](https://pytorch.org/docs/stable/nn.html#loss-functions). | Mean absolute error (MAE) for regression problems ([`torch.nn.L1Loss()`](https://pytorch.org/docs/stable/generated/torch.nn.L1Loss.html)). Binary cross entropy for binary classification problems ([`torch.nn.BCELoss()`](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html)).  |\n",
    "| **Optimizer** | Tells your model how to update its internal parameters to best lower the loss. | You can find various optimization function implementations in [`torch.optim`](https://pytorch.org/docs/stable/optim.html). | Stochastic gradient descent ([`torch.optim.SGD()`](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD)). Adam optimizer ([`torch.optim.Adam()`](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam)). | \n",
    "\n",
    "Let's create a loss function and an optimizer we can use to help improve our model.\n",
    "\n",
    "Depending on what kind of problem you're working on will depend on what loss function and what optimizer you use.\n",
    "\n",
    "However, there are some common values, that are known to work well such as the SGD (stochastic gradient descent) or Adam optimizer. And the MAE (mean absolute error) loss function for regression problems (predicting a number) or binary cross entropy loss function for classification problems (predicting one thing or another). \n",
    "\n",
    "For our problem, since we're predicting a number, let's use MAE (which is under `torch.nn.L1Loss()`) in PyTorch as our loss function.\n",
    "\n",
    "And we'll use SGD, torch.optim.SGD(params, lr) where:\n",
    "\n",
    "params is the target model parameters you'd like to optimize (e.g. the weights and bias values we randomly set before).\n",
    "lr is the learning rate you'd like the optimizer to update the parameters at, higher means the optimizer will try larger updates (these can sometimes be too large and the optimizer will fail to work), lower means the optimizer will try smaller updates (these can sometimes be too small and the optimizer will take too long to find the ideal values). The learning rate is considered a hyperparameter (because it's set by a machine learning engineer). Common starting values for the learning rate are 0.01, 0.001, 0.0001, however, these can also be adjusted over time (this is called learning rate scheduling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup a loss function\n",
    "loss_fn = nn.L1Loss()\n",
    "\n",
    "# Setup an optimizer (stochastic gradient descent)\n",
    "optimizer = torch.optim.SGD(params=model_0.parameters(),   # parameters to optimize \n",
    "                      lr=0.01) # lr: learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### PyTorch training loop\n",
    "For the training loop, we'll build the following steps:\n",
    "\n",
    "| Number | Step name | What does it do? | Code example |\n",
    "| ----- | ----- | ----- | ----- |\n",
    "| 1 | Forward pass | The model goes through all of the training data once, performing its `forward()` function calculations. | `model(x_train)` |\n",
    "| 2 | Calculate the loss | The model's outputs (predictions) are compared to the ground truth and evaluated to see how wrong they are. | `loss = loss_fn(y_pred, y_train)` | \n",
    "| 3 | Zero gradients | The optimizers gradients are set to zero (they are accumulated by default) so they can be recalculated for the specific training step. | `optimizer.zero_grad()` |\n",
    "| 4 | Perform backpropagation on the loss | Computes the gradient of the loss with respect for every model parameter to be updated  (each parameter with `requires_grad=True`). This is known as **backpropagation**, hence \"backwards\".  | `loss.backward()` |\n",
    "| 5 | Update the optimizer (**gradient descent**) | Update the parameters with `requires_grad=True` with respect to the loss gradients in order to improve them. | `optimizer.step()` |\n",
    "\n",
    "Another explanations:\n",
    "\n",
    "0. Loop through data\n",
    "1. Forward pass (this involves data moving through our model's `forward()` function) to make predictions on data - also called forward propagations\n",
    "2. Calculate the loss (compare forward pass predictions to ground truth labels)\n",
    "3. Optimizer zero grad\n",
    "4. Loss backward - move backwards through the network to calculate the gradients of each parameters of our model with respect to the loss\n",
    "5. Optimizer step - use the optimizer to adjust our model's parameters to try and improve the loss (**gradient descent**)\n",
    "\n",
    "![pytorch training loop annotated](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/01-pytorch-training-loop-annotated.png)\n",
    "\n",
    "> **Note:** The above is just one example of how the steps could be ordered or described. With experience you'll find making PyTorch training loops can be quite flexible.\n",
    ">\n",
    "> And on the ordering of things, the above is a good default order but you may see slightly different orders. Some rules of thumb: \n",
    "> * Calculate the loss (`loss = ...`) *before* performing backpropagation on it (`loss.backward()`).\n",
    "> * Zero gradients (`optimizer.zero_grad()`) *before* stepping them (`optimizer.step()`).\n",
    "> * Step the optimizer (`optimizer.step()`) *after* performing backpropagation on the loss (`loss.backward()`).\n",
    "\n",
    "For resources to help understand what's happening behind the scenes with backpropagation and gradient descent, see the extra-curriculum section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 |  Loss: 0.48740957954762054 | Test Loss: 0.7948613474299793\n",
      "Epoch: 10 |  Loss: 0.37219958028567507 | Test Loss: 0.6601513479001855\n",
      "Epoch: 20 |  Loss: 0.25698958102372954 | Test Loss: 0.5254413483703916\n",
      "Epoch: 30 |  Loss: 0.15880869663761565 | Test Loss: 0.40255174883521166\n",
      "Epoch: 40 |  Loss: 0.11402923588994826 | Test Loss: 0.31734834924472094\n",
      "Epoch: 50 |  Loss: 0.0956925529346293 | Test Loss: 0.26267344957748173\n",
      "Epoch: 60 |  Loss: 0.08738441897884866 | Test Loss: 0.22768154985825237\n",
      "Epoch: 70 |  Loss: 0.08259710290923128 | Test Loss: 0.20586695010039308\n",
      "Epoch: 80 |  Loss: 0.07876590153399285 | Test Loss: 0.19077895030586509\n",
      "Epoch: 90 |  Loss: 0.07527590166542106 | Test Loss: 0.17975895045912335\n",
      "Epoch: 100 |  Loss: 0.07178590179684927 | Test Loss: 0.16873895061238167\n",
      "Epoch: 110 |  Loss: 0.06831850502523909 | Test Loss: 0.15909275075797485\n",
      "Epoch: 120 |  Loss: 0.06488270204802468 | Test Loss: 0.15082035089590293\n",
      "Epoch: 130 |  Loss: 0.06144501523411564 | Test Loss: 0.14323485102999844\n",
      "Epoch: 140 |  Loss: 0.05801657533938964 | Test Loss: 0.13496245116792652\n",
      "Epoch: 150 |  Loss: 0.054579102417000494 | Test Loss: 0.1266900513058546\n",
      "Epoch: 160 |  Loss: 0.0511430855482662 | Test Loss: 0.1191045514399501\n",
      "Epoch: 170 |  Loss: 0.047714302662335416 | Test Loss: 0.11083215157787815\n",
      "Epoch: 180 |  Loss: 0.04427550278597629 | Test Loss: 0.1025597517158062\n",
      "Epoch: 190 |  Loss: 0.040841155862416774 | Test Loss: 0.09497425184990174\n",
      "Epoch: 200 |  Loss: 0.03741070303131122 | Test Loss: 0.08670185198782983\n",
      "Epoch: 210 |  Loss: 0.03397190315495212 | Test Loss: 0.07842945212575801\n",
      "Epoch: 220 |  Loss: 0.030539226176567364 | Test Loss: 0.0708439522598535\n",
      "Epoch: 230 |  Loss: 0.027107103400287064 | Test Loss: 0.0625715523977817\n",
      "Epoch: 240 |  Loss: 0.023668303523927965 | Test Loss: 0.05429915253570986\n",
      "Epoch: 250 |  Loss: 0.02023729649071796 | Test Loss: 0.04671365266980534\n",
      "Epoch: 260 |  Loss: 0.016803503769262918 | Test Loss: 0.03844125280773354\n",
      "Epoch: 270 |  Loss: 0.013364703892903812 | Test Loss: 0.03016885294566176\n",
      "Epoch: 280 |  Loss: 0.009935366804868565 | Test Loss: 0.022583353079757184\n",
      "Epoch: 290 |  Loss: 0.006499904138238755 | Test Loss: 0.014310953217685374\n",
      "Epoch: 300 |  Loss: 0.00306187701374513 | Test Loss: 0.0067254533517808746\n",
      "Epoch: 310 |  Loss: 0.007536009114719158 | Test Loss: 0.008337753377971158\n",
      "Epoch: 320 |  Loss: 0.007536009114719158 | Test Loss: 0.008337753377971158\n",
      "Epoch: 330 |  Loss: 0.007536009114719158 | Test Loss: 0.008337753377971158\n",
      "Epoch: 340 |  Loss: 0.007536009114719158 | Test Loss: 0.008337753377971158\n",
      "Epoch: 350 |  Loss: 0.007536009114719158 | Test Loss: 0.008337753377971158\n",
      "Epoch: 360 |  Loss: 0.007536009114719158 | Test Loss: 0.008337753377971158\n",
      "Epoch: 370 |  Loss: 0.007536009114719158 | Test Loss: 0.008337753377971158\n",
      "Epoch: 380 |  Loss: 0.007536009114719158 | Test Loss: 0.008337753377971158\n",
      "Epoch: 390 |  Loss: 0.007536009114719158 | Test Loss: 0.008337753377971158\n",
      "Epoch: 400 |  Loss: 0.007536009114719158 | Test Loss: 0.008337753377971158\n",
      "Epoch: 410 |  Loss: 0.007536009114719158 | Test Loss: 0.008337753377971158\n",
      "Epoch: 420 |  Loss: 0.007536009114719158 | Test Loss: 0.008337753377971158\n",
      "Epoch: 430 |  Loss: 0.007536009114719158 | Test Loss: 0.008337753377971158\n",
      "Epoch: 440 |  Loss: 0.007536009114719158 | Test Loss: 0.008337753377971158\n",
      "Epoch: 450 |  Loss: 0.007536009114719158 | Test Loss: 0.008337753377971158\n",
      "Epoch: 460 |  Loss: 0.007536009114719158 | Test Loss: 0.008337753377971158\n",
      "Epoch: 470 |  Loss: 0.007536009114719158 | Test Loss: 0.008337753377971158\n",
      "Epoch: 480 |  Loss: 0.007536009114719158 | Test Loss: 0.008337753377971158\n",
      "Epoch: 490 |  Loss: 0.007536009114719158 | Test Loss: 0.008337753377971158\n",
      "Epoch: 500 |  Loss: 0.007536009114719158 | Test Loss: 0.008337753377971158\n",
      "Epoch: 510 |  Loss: 0.007536009114719158 | Test Loss: 0.008337753377971158\n",
      "Epoch: 520 |  Loss: 0.007536009114719158 | Test Loss: 0.008337753377971158\n",
      "Epoch: 530 |  Loss: 0.007536009114719158 | Test Loss: 0.008337753377971158\n",
      "Epoch: 540 |  Loss: 0.007536009114719158 | Test Loss: 0.008337753377971158\n",
      "Epoch: 550 |  Loss: 0.007536009114719158 | Test Loss: 0.008337753377971158\n",
      "Epoch: 560 |  Loss: 0.007536009114719158 | Test Loss: 0.008337753377971158\n",
      "Epoch: 570 |  Loss: 0.007536009114719158 | Test Loss: 0.008337753377971158\n",
      "Epoch: 580 |  Loss: 0.007536009114719158 | Test Loss: 0.008337753377971158\n",
      "Epoch: 590 |  Loss: 0.007536009114719158 | Test Loss: 0.008337753377971158\n",
      "Epoch: 600 |  Loss: 0.007536009114719158 | Test Loss: 0.008337753377971158\n",
      "Epoch: 610 |  Loss: 0.007536009114719158 | Test Loss: 0.008337753377971158\n",
      "Epoch: 620 |  Loss: 0.007536009114719158 | Test Loss: 0.008337753377971158\n",
      "Epoch: 630 |  Loss: 0.007536009114719158 | Test Loss: 0.008337753377971158\n",
      "Epoch: 640 |  Loss: 0.007536009114719158 | Test Loss: 0.008337753377971158\n",
      "Epoch: 650 |  Loss: 0.007536009114719158 | Test Loss: 0.008337753377971158\n",
      "Epoch: 660 |  Loss: 0.007536009114719158 | Test Loss: 0.008337753377971158\n",
      "Epoch: 670 |  Loss: 0.007536009114719158 | Test Loss: 0.008337753377971158\n",
      "Epoch: 680 |  Loss: 0.007536009114719158 | Test Loss: 0.008337753377971158\n",
      "Epoch: 690 |  Loss: 0.007536009114719158 | Test Loss: 0.008337753377971158\n",
      "Epoch: 700 |  Loss: 0.007536009114719158 | Test Loss: 0.008337753377971158\n",
      "Epoch: 710 |  Loss: 0.007536009114719158 | Test Loss: 0.008337753377971158\n",
      "Epoch: 720 |  Loss: 0.007536009114719158 | Test Loss: 0.008337753377971158\n",
      "Epoch: 730 |  Loss: 0.007536009114719158 | Test Loss: 0.008337753377971158\n",
      "Epoch: 740 |  Loss: 0.007536009114719158 | Test Loss: 0.008337753377971158\n",
      "Epoch: 750 |  Loss: 0.007536009114719158 | Test Loss: 0.008337753377971158\n",
      "Epoch: 760 |  Loss: 0.007536009114719158 | Test Loss: 0.008337753377971158\n",
      "Epoch: 770 |  Loss: 0.007536009114719158 | Test Loss: 0.008337753377971158\n",
      "Epoch: 780 |  Loss: 0.007536009114719158 | Test Loss: 0.008337753377971158\n",
      "Epoch: 790 |  Loss: 0.007536009114719158 | Test Loss: 0.008337753377971158\n",
      "Epoch: 800 |  Loss: 0.007536009114719158 | Test Loss: 0.008337753377971158\n",
      "Epoch: 810 |  Loss: 0.007536009114719158 | Test Loss: 0.008337753377971158\n",
      "Epoch: 820 |  Loss: 0.007536009114719158 | Test Loss: 0.008337753377971158\n",
      "Epoch: 830 |  Loss: 0.007536009114719158 | Test Loss: 0.008337753377971158\n",
      "Epoch: 840 |  Loss: 0.007536009114719158 | Test Loss: 0.008337753377971158\n",
      "Epoch: 850 |  Loss: 0.007536009114719158 | Test Loss: 0.008337753377971158\n",
      "Epoch: 860 |  Loss: 0.007536009114719158 | Test Loss: 0.008337753377971158\n",
      "Epoch: 870 |  Loss: 0.007536009114719158 | Test Loss: 0.008337753377971158\n",
      "Epoch: 880 |  Loss: 0.007536009114719158 | Test Loss: 0.008337753377971158\n",
      "Epoch: 890 |  Loss: 0.007536009114719158 | Test Loss: 0.008337753377971158\n",
      "Epoch: 900 |  Loss: 0.007536009114719158 | Test Loss: 0.008337753377971158\n",
      "Epoch: 910 |  Loss: 0.007536009114719158 | Test Loss: 0.008337753377971158\n",
      "Epoch: 920 |  Loss: 0.007536009114719158 | Test Loss: 0.008337753377971158\n",
      "Epoch: 930 |  Loss: 0.007536009114719158 | Test Loss: 0.008337753377971158\n",
      "Epoch: 940 |  Loss: 0.007536009114719158 | Test Loss: 0.008337753377971158\n",
      "Epoch: 950 |  Loss: 0.007536009114719158 | Test Loss: 0.008337753377971158\n",
      "Epoch: 960 |  Loss: 0.007536009114719158 | Test Loss: 0.008337753377971158\n",
      "Epoch: 970 |  Loss: 0.007536009114719158 | Test Loss: 0.008337753377971158\n",
      "Epoch: 980 |  Loss: 0.007536009114719158 | Test Loss: 0.008337753377971158\n",
      "Epoch: 990 |  Loss: 0.007536009114719158 | Test Loss: 0.008337753377971158\n"
     ]
    }
   ],
   "source": [
    "# An epoch is one loop through the data... (this is a hyperparameter)\n",
    "epochs = 1000 # an epoch refers to one complete pass through entire dataset\n",
    "\n",
    "# Track different values\n",
    "epoch_count = []\n",
    "loss_values = []\n",
    "test_loss_values = []\n",
    "\n",
    "# 0. Loop through the data\n",
    "for epoch in range(epochs):\n",
    "    # Set model to train mode (this is the default state of the model)\n",
    "    model_0.train() # train mode in PyTorch sets all parameters that require gradients to require gradients\n",
    "    \n",
    "    # 1. Forward pass\n",
    "    y_pred = model_0(X_train)\n",
    "    \n",
    "    # 2. Calculate the loss\n",
    "    loss = loss_fn(y_pred, y_train)\n",
    "         \n",
    "    # 3. Optimizer zero grad\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # 4. Perform backpropagation on the loss with respect to the parameters of the model\n",
    "    loss.backward()\n",
    "    \n",
    "    # 5. Step the optimizer (perform gradient descent)\n",
    "    optimizer.step() # by default the optimizer changes will acumulate through the loop so we have to zero them above in step 3 for the next iteration of the loop\n",
    "    \n",
    "    \n",
    "    ### Testing\n",
    "    model_0.eval() # turns of different settings in the model not needed for evaluation/testing (dropout/batch norm layers, etc.)\n",
    "    with torch.inference_mode(): # (turns of gradient tracking & a couple more things behind)\n",
    "        # 1. Do the forward pass\n",
    "        test_pred = model_0(X_test)\n",
    "        \n",
    "        # 2. Calculate the loss\n",
    "        test_loss = loss_fn(test_pred, y_test)\n",
    "        \n",
    "    # Print out what's happening\n",
    "    if epoch % 10 == 0:\n",
    "        epoch_count.append(epoch)\n",
    "        loss_values.append(loss.detach().numpy())\n",
    "        test_loss_values.append(test_loss.detach().numpy())\n",
    "        print(f\"Epoch: {epoch} |  Loss: {loss} | Test Loss: {test_loss}\") \n",
    "        \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict({'weight': tensor([0.6913], dtype=torch.float64), 'bias': tensor([0.2994], dtype=torch.float64)})\n"
     ]
    }
   ],
   "source": [
    "print(model_0.state_dict()) # Parameters are very close to the desired values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8561],\n",
      "        [0.8699],\n",
      "        [0.8836],\n",
      "        [0.8974],\n",
      "        [0.9112],\n",
      "        [0.9249],\n",
      "        [0.9387],\n",
      "        [0.9525],\n",
      "        [0.9662],\n",
      "        [0.9800]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(epoch_count, loss_values, label=\"Train loss\")\n",
    "plt.plot(epoch_count, test_loss_values, label=\"Test loss\")\n",
    "plt.title(\"Training and test loss curves\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and loading a model\n",
    "\n",
    "We're happy with our models predictions, so let's save it to file so it can be used later.\n",
    "\n",
    "The [recommended way](https://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-loading-model-for-inference) for saving and loading a model for inference (making predictions) is by saving and loading a model's `state_dict()`.\n",
    "\n",
    "Let's see how we can do that in a few steps:\n",
    "\n",
    "1. We'll create a directory for saving models to called `models` using Python's `pathlib` module.\n",
    "2. We'll create a file path to save the model to.\n",
    "3. We'll call `torch.save(obj, f)` where `obj` is the target model's `state_dict()` and `f` is the filename of where to save the model.\n",
    "\n",
    "> **Note:** It's common convention for PyTorch saved models or objects to end with `.pt` or `.pth`, like `saved_model_01.pth`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to: models/01_pytorch_workflow_model_0.pth\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# 1. Create models directory \n",
    "MODEL_PATH = Path(\"models\")\n",
    "MODEL_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 2. Create model save path \n",
    "MODEL_NAME = \"01_pytorch_workflow_model_0.pth\"\n",
    "MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n",
    "\n",
    "# 3. Save the model state dict \n",
    "print(f\"Saving model to: {MODEL_SAVE_PATH}\")\n",
    "torch.save(obj=model_0.state_dict(), # only saving the state_dict() only saves the models learned parameters\n",
    "           f=MODEL_SAVE_PATH) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading a saved PyTorch model's `state_dict()`\n",
    "\n",
    "Since we've now got a saved model `state_dict()` at `models/01_pytorch_workflow_model_0.pth` we can now load it in using `torch.nn.Module.load_state_dict(torch.load(f))` where `f` is the filepath of our saved model `state_dict()`.\n",
    "\n",
    "Why call `torch.load()` inside `torch.nn.Module.load_state_dict()`? \n",
    "\n",
    "Because we only saved the model's `state_dict()` which is a dictionary of learned parameters and not the *entire* model, we first have to load the `state_dict()` with `torch.load()` and then pass that `state_dict()` to a new instance of our model (which is a subclass of `nn.Module`).\n",
    "\n",
    "Why not save the entire model?\n",
    "\n",
    "[Saving the entire model](https://pytorch.org/tutorials/beginner/saving_loading_models.html#save-load-entire-model) rather than just the `state_dict()` is more intuitive, however, to quote the PyTorch documentation (italics mine):\n",
    "\n",
    "> The disadvantage of this approach *(saving the whole model)* is that the serialized data is bound to the specific classes and the exact directory structure used when the model is saved...\n",
    ">\n",
    "> Because of this, your code can break in various ways when used in other projects or after refactors.\n",
    "\n",
    "So instead, we're using the flexible method of saving and loading just the `state_dict()`, which again is basically a dictionary of model parameters.\n",
    "\n",
    "Let's test it out by created another instance of `LinearRegressionModel()`, which is a subclass of `torch.nn.Module` and will hence have the in-built method `load_state_dict()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate a new instance of our model (this will be instantiated with random weights)\n",
    "loaded_model_0 = LinearRegressionModel()\n",
    "\n",
    "# Load the state_dict of our saved model (this will update the new instance of our model with trained weights)\n",
    "loaded_model_0.load_state_dict(torch.load(f=MODEL_SAVE_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Put the loaded model into evaluation mode\n",
    "loaded_model_0.eval()\n",
    "\n",
    "# 2. Use the inference mode context manager to make predictions\n",
    "with torch.inference_mode():\n",
    "    loaded_model_preds = loaded_model_0(X_test) # perform a forward pass on the test data with the loaded model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting all information together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.1\n"
     ]
    }
   ],
   "source": [
    "# Import PyTorch\n",
    "import torch\n",
    "from torch import nn\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Setup device agnostic code\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0000],\n",
       "         [0.0200],\n",
       "         [0.0400],\n",
       "         [0.0600],\n",
       "         [0.0800],\n",
       "         [0.1000],\n",
       "         [0.1200],\n",
       "         [0.1400],\n",
       "         [0.1600],\n",
       "         [0.1800]]),\n",
       " tensor([[0.3000],\n",
       "         [0.3140],\n",
       "         [0.3280],\n",
       "         [0.3420],\n",
       "         [0.3560],\n",
       "         [0.3700],\n",
       "         [0.3840],\n",
       "         [0.3980],\n",
       "         [0.4120],\n",
       "         [0.4260]]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dummy linear data\n",
    "# Create weight and bias\n",
    "weight = 0.7\n",
    "bias = 0.3\n",
    "\n",
    "# Create range values\n",
    "start = 0\n",
    "end = 1\n",
    "step = 0.02\n",
    "\n",
    "# Create X and y (features and labels)\n",
    "X = torch.arange(start, end, step).unsqueeze(dim=1) # without unsqueeze, errors will happen later on (shapes within linear layers)\n",
    "y = weight * X + bias \n",
    "X[:10], y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 40, 10, 10)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split data\n",
    "train_split = int(0.8 * len(X))\n",
    "X_train, y_train = X[:train_split], y[:train_split]\n",
    "X_test, y_test = X[train_split:], y[train_split:]\n",
    "\n",
    "len(X_train), len(y_train), len(X_test), len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `linear layer`, often referred to as a fully connected layer or dense layer in neural networks, applies a linear transformation to the incoming data. It does this by multiplying the input by a weight matrix and then adding a bias vector. This layer does not apply any *non-linearity* or *activation function* on its own; it simply computes linear transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.Linear() her neuron'da sadece linear regresyon modeli kuran bir layer ekler. Eğer in_features ve out_features 1 olarak atanırsa, bunun tek bir input alıp tek bir çıktı dönen tek bir linear regresyon modelinden hiçbir farkı olmaz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Thanks to GPT`\n",
    "Bir nöronun çıktısının vektör olması genel bir durum değildir; aslında, tek bir nöronun çıktısı genellikle tek bir skalar değerdir. Bir katmandaki birden fazla nöronun çıktıları bir araya geldiğinde bir vektör oluşturur, ancak her bir nöron kendi başına yalnızca bir skalar değer üretir. Bu nedenle, bir nöronun çıktısını vektör olarak düşünmek yerine, katman çıktılarını vektör veya matris olarak ele almak daha doğru olacaktır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(LinearRegressionModelV2(\n",
       "   (linear_layer): Linear(in_features=1, out_features=1, bias=True)\n",
       " ),\n",
       " OrderedDict([('linear_layer.weight', tensor([[0.7645]])),\n",
       "              ('linear_layer.bias', tensor([0.8300]))]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LinearRegressionModelV2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Use nn.Linear() for creating the model parameters\n",
    "        self.linear_layer = nn.Linear(in_features=1,\n",
    "                                      out_features=1)\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.linear_layer(x)\n",
    "\n",
    "# Set the manual seed when creating the model (this isn't always need but is used for demonstrative purposes, try commenting it out and seeing what happens)\n",
    "model_1 = LinearRegressionModelV2()\n",
    "model_1, model_1.state_dict() # It will print random initial paramteter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps', index=0)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check model device\n",
    "next(model_1.parameters()).device # cpu\n",
    "model_1.to(device)\n",
    "next(model_1.parameters()).device # mps(gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a loss function\n",
    "loss_fn = nn.L1Loss()\n",
    "# Create a optimizer\n",
    "optimizer = torch.optim.SGD(params=model_1.parameters(),\n",
    "                            lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Train loss: 0.5436569452285767 | Test loss: 0.560505211353302\n",
      "Epoch: 100 | Train loss: 0.0058781662955880165 | Test loss: 0.012984687462449074\n",
      "Epoch: 200 | Train loss: 0.010256483219563961 | Test loss: 0.00033082367735914886\n",
      "Epoch: 300 | Train loss: 0.010256483219563961 | Test loss: 0.00033082367735914886\n",
      "Epoch: 400 | Train loss: 0.010256483219563961 | Test loss: 0.00033082367735914886\n",
      "Epoch: 500 | Train loss: 0.010256483219563961 | Test loss: 0.00033082367735914886\n",
      "Epoch: 600 | Train loss: 0.010256483219563961 | Test loss: 0.00033082367735914886\n",
      "Epoch: 700 | Train loss: 0.010256483219563961 | Test loss: 0.00033082367735914886\n",
      "Epoch: 800 | Train loss: 0.010256483219563961 | Test loss: 0.00033082367735914886\n",
      "Epoch: 900 | Train loss: 0.010256483219563961 | Test loss: 0.00033082367735914886\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# Set the number of epochs\n",
    "epochs = 1000\n",
    "\n",
    "# Data and model should be on the same device (or error appear)\n",
    "X_train = X_train.to(device)\n",
    "X_test = X_test.to(device)\n",
    "y_train = y_train.to(device)\n",
    "y_test = y_test.to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model_1.train()\n",
    "    \n",
    "    # 1. Forward pass\n",
    "    y_pred = model_1(X_train)\n",
    "    \n",
    "    # 2. Calculate loss\n",
    "    loss = loss_fn(y_pred, y_train)\n",
    "    \n",
    "    # 3. Zero grad optimizer\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # 4. Loss backward\n",
    "    loss.backward()\n",
    "    \n",
    "    # 5. Step the optimizer (perform gradient descent)\n",
    "    optimizer.step()\n",
    "    \n",
    "    ### Testing\n",
    "    model_1.eval() # put the model in evaluation mode for testing (inference)\n",
    "    with torch.inference_mode():\n",
    "        test_pred = model_1(X_test)\n",
    "        test_loss = loss_fn(test_pred, y_test)\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch: {epoch} | Train loss: {loss} | Test loss: {test_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
